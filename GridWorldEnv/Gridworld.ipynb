{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import numpy as np\n",
    "import utilities as utils\n",
    "import interval_analysis as ia\n",
    "import neural_nets.train as train\n",
    "import matplotlib.pyplot as plt\n",
    "import neural_nets.data_loaders as data_loaders\n",
    "\n",
    "sys.path.append(\"../../RL_models/\")\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "from lipMIP import LipMIP\n",
    "from hyperbox import Hyperbox\n",
    "from relu_nets import ReLUNet\n",
    "from stable_baselines import DQN\n",
    "from lipschitzModels.DQN import DQN_LipMIP\n",
    "from gridworld.envs.gridworld_env import GridWorldEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv()\n",
    "env.init(episode_step_limit=100)\n",
    "opt = torch.optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to setup: 0.006781339645385742\n"
     ]
    }
   ],
   "source": [
    "model = DQN_LipMIP(env, opt, regularization='l2', verbose=0, model_name='l2_tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:01<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "model.learn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "\n",
      " NEW EPISODE \n",
      "\n",
      "---------------------------\n",
      "Current Position: [99.  0.] \t Current Goal State: [61 14]\n",
      "Step: 1 \t Action: [-1  0]\n",
      "Episode Reward: 0.013331555792560991\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [98.  0.] \t Current Goal State: [61 14]\n",
      "Step: 2 \t Action: [-1  0]\n",
      "Episode Reward: 0.032558627509538494\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [97.  0.] \t Current Goal State: [61 14]\n",
      "Step: 3 \t Action: [-1  0]\n",
      "Episode Reward: 0.052162626725378525\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [96.  0.] \t Current Goal State: [61 14]\n",
      "Step: 4 \t Action: [-1  0]\n",
      "Episode Reward: 0.07215862752521857\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [95.  0.] \t Current Goal State: [61 14]\n",
      "Step: 5 \t Action: [-1  0]\n",
      "Episode Reward: 0.0925626267090586\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [94.  0.] \t Current Goal State: [61 14]\n",
      "Step: 6 \t Action: [-1  0]\n",
      "Episode Reward: 0.11339162066865036\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [93.  0.] \t Current Goal State: [61 14]\n",
      "Step: 7 \t Action: [-1  0]\n",
      "Episode Reward: 0.1346636904410392\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [92.  0.] \t Current Goal State: [61 14]\n",
      "Step: 8 \t Action: [-1  0]\n",
      "Episode Reward: 0.15639809600504703\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [91.  0.] \t Current Goal State: [61 14]\n",
      "Step: 9 \t Action: [-1  0]\n",
      "Episode Reward: 0.1786153810528142\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [90.  0.] \t Current Goal State: [61 14]\n",
      "Step: 10 \t Action: [-1  0]\n",
      "Episode Reward: 0.20133748966449336\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [89.  0.] \t Current Goal State: [61 14]\n",
      "Step: 11 \t Action: [-1  0]\n",
      "Episode Reward: 0.2245878965466138\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [88.  0.] \t Current Goal State: [61 14]\n",
      "Step: 12 \t Action: [-1  0]\n",
      "Episode Reward: 0.2483917527713222\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [87.  0.] \t Current Goal State: [61 14]\n",
      "Step: 13 \t Action: [-1  0]\n",
      "Episode Reward: 0.27277604928436777\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [86.  0.] \t Current Goal State: [61 14]\n",
      "Step: 14 \t Action: [-1  0]\n",
      "Episode Reward: 0.29776980084647725\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [85.  0.] \t Current Goal State: [61 14]\n",
      "Step: 15 \t Action: [-1  0]\n",
      "Episode Reward: 0.323404253550912\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [84.  0.] \t Current Goal State: [61 14]\n",
      "Step: 16 \t Action: [-1  0]\n",
      "Episode Reward: 0.3497131196387836\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [83.  0.] \t Current Goal State: [61 14]\n",
      "Step: 17 \t Action: [-1  0]\n",
      "Episode Reward: 0.3767328440375947\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [82.  0.] \t Current Goal State: [61 14]\n",
      "Step: 18 \t Action: [-1  0]\n",
      "Episode Reward: 0.4045029079087416\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [81.  0.] \t Current Goal State: [61 14]\n",
      "Step: 19 \t Action: [-1  0]\n",
      "Episode Reward: 0.43306617554655935\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [80.  0.] \t Current Goal State: [61 14]\n",
      "Step: 20 \t Action: [-1  0]\n",
      "Episode Reward: 0.4624692922769328\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [79.  0.] \t Current Goal State: [61 14]\n",
      "Step: 21 \t Action: [-1  0]\n",
      "Episode Reward: 0.49276314262531207\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [78.  0.] \t Current Goal State: [61 14]\n",
      "Step: 22 \t Action: [-1  0]\n",
      "Episode Reward: 0.5240033800511165\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [77.  0.] \t Current Goal State: [61 14]\n",
      "Step: 23 \t Action: [-1  0]\n",
      "Episode Reward: 0.5562510420956183\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [76.  0.] \t Current Goal State: [61 14]\n",
      "Step: 24 \t Action: [-1  0]\n",
      "Episode Reward: 0.5895732680203101\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [75.  0.] \t Current Goal State: [61 14]\n",
      "Step: 25 \t Action: [-1  0]\n",
      "Episode Reward: 0.6240441401333745\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [74.  0.] \t Current Goal State: [61 14]\n",
      "Step: 26 \t Action: [-1  0]\n",
      "Episode Reward: 0.6597456752993867\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [73.  0.] \t Current Goal State: [61 14]\n",
      "Step: 27 \t Action: [-1  0]\n",
      "Episode Reward: 0.6967689999939443\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [72.  0.] \t Current Goal State: [61 14]\n",
      "Step: 28 \t Action: [-1  0]\n",
      "Episode Reward: 0.7352157512434637\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [71.  0.] \t Current Goal State: [61 14]\n",
      "Step: 29 \t Action: [-1  0]\n",
      "Episode Reward: 0.7751997576409048\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [70.  0.] \t Current Goal State: [61 14]\n",
      "Step: 30 \t Action: [-1  0]\n",
      "Episode Reward: 0.8168490704272438\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [69.  0.] \t Current Goal State: [61 14]\n",
      "Step: 31 \t Action: [-1  0]\n",
      "Episode Reward: 0.8603084359205075\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [68.  0.] \t Current Goal State: [61 14]\n",
      "Step: 32 \t Action: [-1  0]\n",
      "Episode Reward: 0.9057423296051963\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [67.  0.] \t Current Goal State: [61 14]\n",
      "Step: 33 \t Action: [-1  0]\n",
      "Episode Reward: 0.953338712280113\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [66.  0.] \t Current Goal State: [61 14]\n",
      "Step: 34 \t Action: [-1  0]\n",
      "Episode Reward: 1.0033137247738662\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [65.  0.] \t Current Goal State: [61 14]\n",
      "Step: 35 \t Action: [-1  0]\n",
      "Episode Reward: 1.0559176174619251\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [64.  0.] \t Current Goal State: [61 14]\n",
      "Step: 36 \t Action: [-1  0]\n",
      "Episode Reward: 1.1114423259572055\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [63.  0.] \t Current Goal State: [61 14]\n",
      "Step: 37 \t Action: [-1  0]\n",
      "Episode Reward: 1.170231273635042\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [62.  0.] \t Current Goal State: [61 14]\n",
      "Step: 38 \t Action: [-1  0]\n",
      "Episode Reward: 1.2326922355338552\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [61.  0.] \t Current Goal State: [61 14]\n",
      "Step: 39 \t Action: [-1  0]\n",
      "Episode Reward: 1.2993144873659672\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [60.  0.] \t Current Goal State: [61 14]\n",
      "Step: 40 \t Action: [-1  0]\n",
      "Episode Reward: 1.3706920748035119\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [59.  0.] \t Current Goal State: [61 14]\n",
      "Step: 41 \t Action: [-1  0]\n",
      "Episode Reward: 1.4373143266356239\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [58.  0.] \t Current Goal State: [61 14]\n",
      "Step: 42 \t Action: [-1  0]\n",
      "Episode Reward: 1.499775288534437\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [57.  0.] \t Current Goal State: [61 14]\n",
      "Step: 43 \t Action: [-1  0]\n",
      "Episode Reward: 1.5585642362122736\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [56.  0.] \t Current Goal State: [61 14]\n",
      "Step: 44 \t Action: [-1  0]\n",
      "Episode Reward: 1.614088944707554\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [55.  0.] \t Current Goal State: [61 14]\n",
      "Step: 45 \t Action: [-1  0]\n",
      "Episode Reward: 1.666692837395613\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [54.  0.] \t Current Goal State: [61 14]\n",
      "Step: 46 \t Action: [-1  0]\n",
      "Episode Reward: 1.716667849889366\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [53.  0.] \t Current Goal State: [61 14]\n",
      "Step: 47 \t Action: [-1  0]\n",
      "Episode Reward: 1.7642642325642826\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [52.  0.] \t Current Goal State: [61 14]\n",
      "Step: 48 \t Action: [-1  0]\n",
      "Episode Reward: 1.8096981262489713\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [51.  0.] \t Current Goal State: [61 14]\n",
      "Step: 49 \t Action: [-1  0]\n",
      "Episode Reward: 1.853157491742235\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [50.  0.] \t Current Goal State: [61 14]\n",
      "Step: 50 \t Action: [-1  0]\n",
      "Episode Reward: 1.8948068045285742\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [49.  0.] \t Current Goal State: [61 14]\n",
      "Step: 51 \t Action: [-1  0]\n",
      "Episode Reward: 1.9347908109260152\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [48.  0.] \t Current Goal State: [61 14]\n",
      "Step: 52 \t Action: [-1  0]\n",
      "Episode Reward: 1.9732375621755347\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [47.  0.] \t Current Goal State: [61 14]\n",
      "Step: 53 \t Action: [-1  0]\n",
      "Episode Reward: 2.010260886870092\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [46.  0.] \t Current Goal State: [61 14]\n",
      "Step: 54 \t Action: [-1  0]\n",
      "Episode Reward: 2.045962422036104\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [45.  0.] \t Current Goal State: [61 14]\n",
      "Step: 55 \t Action: [-1  0]\n",
      "Episode Reward: 2.0804332941491688\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [44.  0.] \t Current Goal State: [61 14]\n",
      "Step: 56 \t Action: [-1  0]\n",
      "Episode Reward: 2.1137555200738607\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [43.  0.] \t Current Goal State: [61 14]\n",
      "Step: 57 \t Action: [-1  0]\n",
      "Episode Reward: 2.1460031821183625\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [42.  0.] \t Current Goal State: [61 14]\n",
      "Step: 58 \t Action: [-1  0]\n",
      "Episode Reward: 2.177243419544167\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [41.  0.] \t Current Goal State: [61 14]\n",
      "Step: 59 \t Action: [-1  0]\n",
      "Episode Reward: 2.207537269892546\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [40.  0.] \t Current Goal State: [61 14]\n",
      "Step: 60 \t Action: [-1  0]\n",
      "Episode Reward: 2.236940386622919\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [39.  0.] \t Current Goal State: [61 14]\n",
      "Step: 61 \t Action: [-1  0]\n",
      "Episode Reward: 2.265503654260737\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [38.  0.] \t Current Goal State: [61 14]\n",
      "Step: 62 \t Action: [-1  0]\n",
      "Episode Reward: 2.293273718131884\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [37.  0.] \t Current Goal State: [61 14]\n",
      "Step: 63 \t Action: [-1  0]\n",
      "Episode Reward: 2.320293442530695\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [36.  0.] \t Current Goal State: [61 14]\n",
      "Step: 64 \t Action: [-1  0]\n",
      "Episode Reward: 2.3466023086185666\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [35.  0.] \t Current Goal State: [61 14]\n",
      "Step: 65 \t Action: [-1  0]\n",
      "Episode Reward: 2.372236761323001\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [34.  0.] \t Current Goal State: [61 14]\n",
      "Step: 66 \t Action: [-1  0]\n",
      "Episode Reward: 2.3972305128851104\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [33.  0.] \t Current Goal State: [61 14]\n",
      "Step: 67 \t Action: [-1  0]\n",
      "Episode Reward: 2.421614809398156\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [32.  0.] \t Current Goal State: [61 14]\n",
      "Step: 68 \t Action: [-1  0]\n",
      "Episode Reward: 2.4454186656228645\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [31.  0.] \t Current Goal State: [61 14]\n",
      "Step: 69 \t Action: [-1  0]\n",
      "Episode Reward: 2.468669072504985\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [30.  0.] \t Current Goal State: [61 14]\n",
      "Step: 70 \t Action: [-1  0]\n",
      "Episode Reward: 2.491391181116664\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [29.  0.] \t Current Goal State: [61 14]\n",
      "Step: 71 \t Action: [-1  0]\n",
      "Episode Reward: 2.513608466164431\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [28.  0.] \t Current Goal State: [61 14]\n",
      "Step: 72 \t Action: [-1  0]\n",
      "Episode Reward: 2.5353428717284388\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [27.  0.] \t Current Goal State: [61 14]\n",
      "Step: 73 \t Action: [-1  0]\n",
      "Episode Reward: 2.5566149415008277\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [26.  0.] \t Current Goal State: [61 14]\n",
      "Step: 74 \t Action: [-1  0]\n",
      "Episode Reward: 2.5774439354604195\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [25.  0.] \t Current Goal State: [61 14]\n",
      "Step: 75 \t Action: [-1  0]\n",
      "Episode Reward: 2.5978479346442596\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [24.  0.] \t Current Goal State: [61 14]\n",
      "Step: 76 \t Action: [-1  0]\n",
      "Episode Reward: 2.6178439354440997\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [23.  0.] \t Current Goal State: [61 14]\n",
      "Step: 77 \t Action: [-1  0]\n",
      "Episode Reward: 2.6374479346599395\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [22.  0.] \t Current Goal State: [61 14]\n",
      "Step: 78 \t Action: [-1  0]\n",
      "Episode Reward: 2.656675006376917\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [21.  0.] \t Current Goal State: [61 14]\n",
      "Step: 79 \t Action: [-1  0]\n",
      "Episode Reward: 2.675539371591028\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [20.  0.] \t Current Goal State: [61 14]\n",
      "Step: 80 \t Action: [-1  0]\n",
      "Episode Reward: 2.6940544613892135\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [19.  0.] \t Current Goal State: [61 14]\n",
      "Step: 81 \t Action: [-1  0]\n",
      "Episode Reward: 2.71223297438685\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [18.  0.] \t Current Goal State: [61 14]\n",
      "Step: 82 \t Action: [-1  0]\n",
      "Episode Reward: 2.7300869290378054\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [17.  0.] \t Current Goal State: [61 14]\n",
      "Step: 83 \t Action: [-1  0]\n",
      "Episode Reward: 2.7476277113566967\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [16.  0.] \t Current Goal State: [61 14]\n",
      "Step: 84 \t Action: [-1  0]\n",
      "Episode Reward: 2.7648661185278742\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [15.  0.] \t Current Goal State: [61 14]\n",
      "Step: 85 \t Action: [-1  0]\n",
      "Episode Reward: 2.7818123988193504\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [14.  0.] \t Current Goal State: [61 14]\n",
      "Step: 86 \t Action: [-1  0]\n",
      "Episode Reward: 2.798476288171125\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [13.  0.] \t Current Goal State: [61 14]\n",
      "Step: 87 \t Action: [-1  0]\n",
      "Episode Reward: 2.8148670437849588\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [12.  0.] \t Current Goal State: [61 14]\n",
      "Step: 88 \t Action: [-1  0]\n",
      "Episode Reward: 2.8309934750057297\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [11.  0.] \t Current Goal State: [61 14]\n",
      "Step: 89 \t Action: [-1  0]\n",
      "Episode Reward: 2.8468639717522777\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [10.  0.] \t Current Goal State: [61 14]\n",
      "Step: 90 \t Action: [-1  0]\n",
      "Episode Reward: 2.8624865307274376\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [9. 0.] \t Current Goal State: [61 14]\n",
      "Step: 91 \t Action: [-1  0]\n",
      "Episode Reward: 2.8778687796122244\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [8. 0.] \t Current Goal State: [61 14]\n",
      "Step: 92 \t Action: [-1  0]\n",
      "Episode Reward: 2.893017999427404\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [7. 0.] \t Current Goal State: [61 14]\n",
      "Step: 93 \t Action: [-1  0]\n",
      "Episode Reward: 2.9079411452265385\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [6. 0.] \t Current Goal State: [61 14]\n",
      "Step: 94 \t Action: [-1  0]\n",
      "Episode Reward: 2.922644865267709\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [5. 0.] \t Current Goal State: [61 14]\n",
      "Step: 95 \t Action: [-1  0]\n",
      "Episode Reward: 2.9371355187961834\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [4. 0.] \t Current Goal State: [61 14]\n",
      "Step: 96 \t Action: [-1  0]\n",
      "Episode Reward: 2.9514191925570747\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [3. 0.] \t Current Goal State: [61 14]\n",
      "Step: 97 \t Action: [-1  0]\n",
      "Episode Reward: 2.965501716145302\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [2. 0.] \t Current Goal State: [61 14]\n",
      "Step: 98 \t Action: [-1  0]\n",
      "Episode Reward: 2.979388676289726\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [1. 0.] \t Current Goal State: [61 14]\n",
      "Step: 99 \t Action: [-1  0]\n",
      "Episode Reward: 2.9930854301590593\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [0. 0.] \t Current Goal State: [61 14]\n",
      "Step: 100 \t Action: [-1  0]\n",
      "Episode Reward: 3.0065971177688415\n",
      "--------------------------------------------------------------------------------------\n",
      "---------------------------\n",
      "\n",
      " NEW EPISODE \n",
      "\n",
      "---------------------------\n",
      "Current Position: [99.  0.] \t Current Goal State: [61 14]\n",
      "Step: 1 \t Action: [-1  0]\n",
      "Episode Reward: 0.013331555792560991\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [98.  0.] \t Current Goal State: [61 14]\n",
      "Step: 2 \t Action: [-1  0]\n",
      "Episode Reward: 0.032558627509538494\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [97.  0.] \t Current Goal State: [61 14]\n",
      "Step: 3 \t Action: [-1  0]\n",
      "Episode Reward: 0.052162626725378525\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [96.  0.] \t Current Goal State: [61 14]\n",
      "Step: 4 \t Action: [-1  0]\n",
      "Episode Reward: 0.07215862752521857\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [95.  0.] \t Current Goal State: [61 14]\n",
      "Step: 5 \t Action: [-1  0]\n",
      "Episode Reward: 0.0925626267090586\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [94.  0.] \t Current Goal State: [61 14]\n",
      "Step: 6 \t Action: [-1  0]\n",
      "Episode Reward: 0.11339162066865036\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [93.  0.] \t Current Goal State: [61 14]\n",
      "Step: 7 \t Action: [-1  0]\n",
      "Episode Reward: 0.1346636904410392\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [92.  0.] \t Current Goal State: [61 14]\n",
      "Step: 8 \t Action: [-1  0]\n",
      "Episode Reward: 0.15639809600504703\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [91.  0.] \t Current Goal State: [61 14]\n",
      "Step: 9 \t Action: [-1  0]\n",
      "Episode Reward: 0.1786153810528142\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [90.  0.] \t Current Goal State: [61 14]\n",
      "Step: 10 \t Action: [-1  0]\n",
      "Episode Reward: 0.20133748966449336\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [89.  0.] \t Current Goal State: [61 14]\n",
      "Step: 11 \t Action: [-1  0]\n",
      "Episode Reward: 0.2245878965466138\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [88.  0.] \t Current Goal State: [61 14]\n",
      "Step: 12 \t Action: [-1  0]\n",
      "Episode Reward: 0.2483917527713222\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [87.  0.] \t Current Goal State: [61 14]\n",
      "Step: 13 \t Action: [-1  0]\n",
      "Episode Reward: 0.27277604928436777\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [86.  0.] \t Current Goal State: [61 14]\n",
      "Step: 14 \t Action: [-1  0]\n",
      "Episode Reward: 0.29776980084647725\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [85.  0.] \t Current Goal State: [61 14]\n",
      "Step: 15 \t Action: [-1  0]\n",
      "Episode Reward: 0.323404253550912\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [84.  0.] \t Current Goal State: [61 14]\n",
      "Step: 16 \t Action: [-1  0]\n",
      "Episode Reward: 0.3497131196387836\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [83.  0.] \t Current Goal State: [61 14]\n",
      "Step: 17 \t Action: [-1  0]\n",
      "Episode Reward: 0.3767328440375947\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [82.  0.] \t Current Goal State: [61 14]\n",
      "Step: 18 \t Action: [-1  0]\n",
      "Episode Reward: 0.4045029079087416\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [81.  0.] \t Current Goal State: [61 14]\n",
      "Step: 19 \t Action: [-1  0]\n",
      "Episode Reward: 0.43306617554655935\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [80.  0.] \t Current Goal State: [61 14]\n",
      "Step: 20 \t Action: [-1  0]\n",
      "Episode Reward: 0.4624692922769328\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [79.  0.] \t Current Goal State: [61 14]\n",
      "Step: 21 \t Action: [-1  0]\n",
      "Episode Reward: 0.49276314262531207\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [78.  0.] \t Current Goal State: [61 14]\n",
      "Step: 22 \t Action: [-1  0]\n",
      "Episode Reward: 0.5240033800511165\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [77.  0.] \t Current Goal State: [61 14]\n",
      "Step: 23 \t Action: [-1  0]\n",
      "Episode Reward: 0.5562510420956183\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [76.  0.] \t Current Goal State: [61 14]\n",
      "Step: 24 \t Action: [-1  0]\n",
      "Episode Reward: 0.5895732680203101\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [75.  0.] \t Current Goal State: [61 14]\n",
      "Step: 25 \t Action: [-1  0]\n",
      "Episode Reward: 0.6240441401333745\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [74.  0.] \t Current Goal State: [61 14]\n",
      "Step: 26 \t Action: [-1  0]\n",
      "Episode Reward: 0.6597456752993867\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [73.  0.] \t Current Goal State: [61 14]\n",
      "Step: 27 \t Action: [-1  0]\n",
      "Episode Reward: 0.6967689999939443\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [72.  0.] \t Current Goal State: [61 14]\n",
      "Step: 28 \t Action: [-1  0]\n",
      "Episode Reward: 0.7352157512434637\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [71.  0.] \t Current Goal State: [61 14]\n",
      "Step: 29 \t Action: [-1  0]\n",
      "Episode Reward: 0.7751997576409048\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [70.  0.] \t Current Goal State: [61 14]\n",
      "Step: 30 \t Action: [-1  0]\n",
      "Episode Reward: 0.8168490704272438\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [69.  0.] \t Current Goal State: [61 14]\n",
      "Step: 31 \t Action: [-1  0]\n",
      "Episode Reward: 0.8603084359205075\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [68.  0.] \t Current Goal State: [61 14]\n",
      "Step: 32 \t Action: [-1  0]\n",
      "Episode Reward: 0.9057423296051963\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [67.  0.] \t Current Goal State: [61 14]\n",
      "Step: 33 \t Action: [-1  0]\n",
      "Episode Reward: 0.953338712280113\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [66.  0.] \t Current Goal State: [61 14]\n",
      "Step: 34 \t Action: [-1  0]\n",
      "Episode Reward: 1.0033137247738662\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [65.  0.] \t Current Goal State: [61 14]\n",
      "Step: 35 \t Action: [-1  0]\n",
      "Episode Reward: 1.0559176174619251\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [64.  0.] \t Current Goal State: [61 14]\n",
      "Step: 36 \t Action: [-1  0]\n",
      "Episode Reward: 1.1114423259572055\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [63.  0.] \t Current Goal State: [61 14]\n",
      "Step: 37 \t Action: [-1  0]\n",
      "Episode Reward: 1.170231273635042\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [62.  0.] \t Current Goal State: [61 14]\n",
      "Step: 38 \t Action: [-1  0]\n",
      "Episode Reward: 1.2326922355338552\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [61.  0.] \t Current Goal State: [61 14]\n",
      "Step: 39 \t Action: [-1  0]\n",
      "Episode Reward: 1.2993144873659672\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [60.  0.] \t Current Goal State: [61 14]\n",
      "Step: 40 \t Action: [-1  0]\n",
      "Episode Reward: 1.3706920748035119\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [59.  0.] \t Current Goal State: [61 14]\n",
      "Step: 41 \t Action: [-1  0]\n",
      "Episode Reward: 1.4373143266356239\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [58.  0.] \t Current Goal State: [61 14]\n",
      "Step: 42 \t Action: [-1  0]\n",
      "Episode Reward: 1.499775288534437\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [57.  0.] \t Current Goal State: [61 14]\n",
      "Step: 43 \t Action: [-1  0]\n",
      "Episode Reward: 1.5585642362122736\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [56.  0.] \t Current Goal State: [61 14]\n",
      "Step: 44 \t Action: [-1  0]\n",
      "Episode Reward: 1.614088944707554\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [55.  0.] \t Current Goal State: [61 14]\n",
      "Step: 45 \t Action: [-1  0]\n",
      "Episode Reward: 1.666692837395613\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [54.  0.] \t Current Goal State: [61 14]\n",
      "Step: 46 \t Action: [-1  0]\n",
      "Episode Reward: 1.716667849889366\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [53.  0.] \t Current Goal State: [61 14]\n",
      "Step: 47 \t Action: [-1  0]\n",
      "Episode Reward: 1.7642642325642826\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [52.  0.] \t Current Goal State: [61 14]\n",
      "Step: 48 \t Action: [-1  0]\n",
      "Episode Reward: 1.8096981262489713\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [51.  0.] \t Current Goal State: [61 14]\n",
      "Step: 49 \t Action: [-1  0]\n",
      "Episode Reward: 1.853157491742235\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [50.  0.] \t Current Goal State: [61 14]\n",
      "Step: 50 \t Action: [-1  0]\n",
      "Episode Reward: 1.8948068045285742\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [49.  0.] \t Current Goal State: [61 14]\n",
      "Step: 51 \t Action: [-1  0]\n",
      "Episode Reward: 1.9347908109260152\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [48.  0.] \t Current Goal State: [61 14]\n",
      "Step: 52 \t Action: [-1  0]\n",
      "Episode Reward: 1.9732375621755347\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [47.  0.] \t Current Goal State: [61 14]\n",
      "Step: 53 \t Action: [-1  0]\n",
      "Episode Reward: 2.010260886870092\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [46.  0.] \t Current Goal State: [61 14]\n",
      "Step: 54 \t Action: [-1  0]\n",
      "Episode Reward: 2.045962422036104\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [45.  0.] \t Current Goal State: [61 14]\n",
      "Step: 55 \t Action: [-1  0]\n",
      "Episode Reward: 2.0804332941491688\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [44.  0.] \t Current Goal State: [61 14]\n",
      "Step: 56 \t Action: [-1  0]\n",
      "Episode Reward: 2.1137555200738607\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [43.  0.] \t Current Goal State: [61 14]\n",
      "Step: 57 \t Action: [-1  0]\n",
      "Episode Reward: 2.1460031821183625\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [42.  0.] \t Current Goal State: [61 14]\n",
      "Step: 58 \t Action: [-1  0]\n",
      "Episode Reward: 2.177243419544167\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [41.  0.] \t Current Goal State: [61 14]\n",
      "Step: 59 \t Action: [-1  0]\n",
      "Episode Reward: 2.207537269892546\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [40.  0.] \t Current Goal State: [61 14]\n",
      "Step: 60 \t Action: [-1  0]\n",
      "Episode Reward: 2.236940386622919\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [39.  0.] \t Current Goal State: [61 14]\n",
      "Step: 61 \t Action: [-1  0]\n",
      "Episode Reward: 2.265503654260737\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [38.  0.] \t Current Goal State: [61 14]\n",
      "Step: 62 \t Action: [-1  0]\n",
      "Episode Reward: 2.293273718131884\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [37.  0.] \t Current Goal State: [61 14]\n",
      "Step: 63 \t Action: [-1  0]\n",
      "Episode Reward: 2.320293442530695\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [36.  0.] \t Current Goal State: [61 14]\n",
      "Step: 64 \t Action: [-1  0]\n",
      "Episode Reward: 2.3466023086185666\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [35.  0.] \t Current Goal State: [61 14]\n",
      "Step: 65 \t Action: [-1  0]\n",
      "Episode Reward: 2.372236761323001\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [34.  0.] \t Current Goal State: [61 14]\n",
      "Step: 66 \t Action: [-1  0]\n",
      "Episode Reward: 2.3972305128851104\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [33.  0.] \t Current Goal State: [61 14]\n",
      "Step: 67 \t Action: [-1  0]\n",
      "Episode Reward: 2.421614809398156\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [32.  0.] \t Current Goal State: [61 14]\n",
      "Step: 68 \t Action: [-1  0]\n",
      "Episode Reward: 2.4454186656228645\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [31.  0.] \t Current Goal State: [61 14]\n",
      "Step: 69 \t Action: [-1  0]\n",
      "Episode Reward: 2.468669072504985\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [30.  0.] \t Current Goal State: [61 14]\n",
      "Step: 70 \t Action: [-1  0]\n",
      "Episode Reward: 2.491391181116664\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [29.  0.] \t Current Goal State: [61 14]\n",
      "Step: 71 \t Action: [-1  0]\n",
      "Episode Reward: 2.513608466164431\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [28.  0.] \t Current Goal State: [61 14]\n",
      "Step: 72 \t Action: [-1  0]\n",
      "Episode Reward: 2.5353428717284388\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [27.  0.] \t Current Goal State: [61 14]\n",
      "Step: 73 \t Action: [-1  0]\n",
      "Episode Reward: 2.5566149415008277\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [26.  0.] \t Current Goal State: [61 14]\n",
      "Step: 74 \t Action: [-1  0]\n",
      "Episode Reward: 2.5774439354604195\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [25.  0.] \t Current Goal State: [61 14]\n",
      "Step: 75 \t Action: [-1  0]\n",
      "Episode Reward: 2.5978479346442596\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [24.  0.] \t Current Goal State: [61 14]\n",
      "Step: 76 \t Action: [-1  0]\n",
      "Episode Reward: 2.6178439354440997\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [23.  0.] \t Current Goal State: [61 14]\n",
      "Step: 77 \t Action: [-1  0]\n",
      "Episode Reward: 2.6374479346599395\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [22.  0.] \t Current Goal State: [61 14]\n",
      "Step: 78 \t Action: [-1  0]\n",
      "Episode Reward: 2.656675006376917\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [21.  0.] \t Current Goal State: [61 14]\n",
      "Step: 79 \t Action: [-1  0]\n",
      "Episode Reward: 2.675539371591028\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [20.  0.] \t Current Goal State: [61 14]\n",
      "Step: 80 \t Action: [-1  0]\n",
      "Episode Reward: 2.6940544613892135\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [19.  0.] \t Current Goal State: [61 14]\n",
      "Step: 81 \t Action: [-1  0]\n",
      "Episode Reward: 2.71223297438685\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [18.  0.] \t Current Goal State: [61 14]\n",
      "Step: 82 \t Action: [-1  0]\n",
      "Episode Reward: 2.7300869290378054\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [17.  0.] \t Current Goal State: [61 14]\n",
      "Step: 83 \t Action: [-1  0]\n",
      "Episode Reward: 2.7476277113566967\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [16.  0.] \t Current Goal State: [61 14]\n",
      "Step: 84 \t Action: [-1  0]\n",
      "Episode Reward: 2.7648661185278742\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [15.  0.] \t Current Goal State: [61 14]\n",
      "Step: 85 \t Action: [-1  0]\n",
      "Episode Reward: 2.7818123988193504\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [14.  0.] \t Current Goal State: [61 14]\n",
      "Step: 86 \t Action: [-1  0]\n",
      "Episode Reward: 2.798476288171125\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [13.  0.] \t Current Goal State: [61 14]\n",
      "Step: 87 \t Action: [-1  0]\n",
      "Episode Reward: 2.8148670437849588\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [12.  0.] \t Current Goal State: [61 14]\n",
      "Step: 88 \t Action: [-1  0]\n",
      "Episode Reward: 2.8309934750057297\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [11.  0.] \t Current Goal State: [61 14]\n",
      "Step: 89 \t Action: [-1  0]\n",
      "Episode Reward: 2.8468639717522777\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [10.  0.] \t Current Goal State: [61 14]\n",
      "Step: 90 \t Action: [-1  0]\n",
      "Episode Reward: 2.8624865307274376\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [9. 0.] \t Current Goal State: [61 14]\n",
      "Step: 91 \t Action: [-1  0]\n",
      "Episode Reward: 2.8778687796122244\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [8. 0.] \t Current Goal State: [61 14]\n",
      "Step: 92 \t Action: [-1  0]\n",
      "Episode Reward: 2.893017999427404\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [7. 0.] \t Current Goal State: [61 14]\n",
      "Step: 93 \t Action: [-1  0]\n",
      "Episode Reward: 2.9079411452265385\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [6. 0.] \t Current Goal State: [61 14]\n",
      "Step: 94 \t Action: [-1  0]\n",
      "Episode Reward: 2.922644865267709\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [5. 0.] \t Current Goal State: [61 14]\n",
      "Step: 95 \t Action: [-1  0]\n",
      "Episode Reward: 2.9371355187961834\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [4. 0.] \t Current Goal State: [61 14]\n",
      "Step: 96 \t Action: [-1  0]\n",
      "Episode Reward: 2.9514191925570747\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [3. 0.] \t Current Goal State: [61 14]\n",
      "Step: 97 \t Action: [-1  0]\n",
      "Episode Reward: 2.965501716145302\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [2. 0.] \t Current Goal State: [61 14]\n",
      "Step: 98 \t Action: [-1  0]\n",
      "Episode Reward: 2.979388676289726\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [1. 0.] \t Current Goal State: [61 14]\n",
      "Step: 99 \t Action: [-1  0]\n",
      "Episode Reward: 2.9930854301590593\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [0. 0.] \t Current Goal State: [61 14]\n",
      "Step: 100 \t Action: [-1  0]\n",
      "Episode Reward: 3.0065971177688415\n",
      "--------------------------------------------------------------------------------------\n",
      "---------------------------\n",
      "\n",
      " NEW EPISODE \n",
      "\n",
      "---------------------------\n",
      "Current Position: [99.  0.] \t Current Goal State: [61 14]\n",
      "Step: 1 \t Action: [-1  0]\n",
      "Episode Reward: 0.013331555792560991\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [98.  0.] \t Current Goal State: [61 14]\n",
      "Step: 2 \t Action: [-1  0]\n",
      "Episode Reward: 0.032558627509538494\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [97.  0.] \t Current Goal State: [61 14]\n",
      "Step: 3 \t Action: [-1  0]\n",
      "Episode Reward: 0.052162626725378525\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [96.  0.] \t Current Goal State: [61 14]\n",
      "Step: 4 \t Action: [-1  0]\n",
      "Episode Reward: 0.07215862752521857\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [95.  0.] \t Current Goal State: [61 14]\n",
      "Step: 5 \t Action: [-1  0]\n",
      "Episode Reward: 0.0925626267090586\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [94.  0.] \t Current Goal State: [61 14]\n",
      "Step: 6 \t Action: [-1  0]\n",
      "Episode Reward: 0.11339162066865036\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [93.  0.] \t Current Goal State: [61 14]\n",
      "Step: 7 \t Action: [-1  0]\n",
      "Episode Reward: 0.1346636904410392\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [92.  0.] \t Current Goal State: [61 14]\n",
      "Step: 8 \t Action: [-1  0]\n",
      "Episode Reward: 0.15639809600504703\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [91.  0.] \t Current Goal State: [61 14]\n",
      "Step: 9 \t Action: [-1  0]\n",
      "Episode Reward: 0.1786153810528142\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [90.  0.] \t Current Goal State: [61 14]\n",
      "Step: 10 \t Action: [-1  0]\n",
      "Episode Reward: 0.20133748966449336\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [89.  0.] \t Current Goal State: [61 14]\n",
      "Step: 11 \t Action: [-1  0]\n",
      "Episode Reward: 0.2245878965466138\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [88.  0.] \t Current Goal State: [61 14]\n",
      "Step: 12 \t Action: [-1  0]\n",
      "Episode Reward: 0.2483917527713222\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [87.  0.] \t Current Goal State: [61 14]\n",
      "Step: 13 \t Action: [-1  0]\n",
      "Episode Reward: 0.27277604928436777\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [86.  0.] \t Current Goal State: [61 14]\n",
      "Step: 14 \t Action: [-1  0]\n",
      "Episode Reward: 0.29776980084647725\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [85.  0.] \t Current Goal State: [61 14]\n",
      "Step: 15 \t Action: [-1  0]\n",
      "Episode Reward: 0.323404253550912\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [84.  0.] \t Current Goal State: [61 14]\n",
      "Step: 16 \t Action: [-1  0]\n",
      "Episode Reward: 0.3497131196387836\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [83.  0.] \t Current Goal State: [61 14]\n",
      "Step: 17 \t Action: [-1  0]\n",
      "Episode Reward: 0.3767328440375947\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [82.  0.] \t Current Goal State: [61 14]\n",
      "Step: 18 \t Action: [-1  0]\n",
      "Episode Reward: 0.4045029079087416\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [81.  0.] \t Current Goal State: [61 14]\n",
      "Step: 19 \t Action: [-1  0]\n",
      "Episode Reward: 0.43306617554655935\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [80.  0.] \t Current Goal State: [61 14]\n",
      "Step: 20 \t Action: [-1  0]\n",
      "Episode Reward: 0.4624692922769328\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [79.  0.] \t Current Goal State: [61 14]\n",
      "Step: 21 \t Action: [-1  0]\n",
      "Episode Reward: 0.49276314262531207\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [78.  0.] \t Current Goal State: [61 14]\n",
      "Step: 22 \t Action: [-1  0]\n",
      "Episode Reward: 0.5240033800511165\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [77.  0.] \t Current Goal State: [61 14]\n",
      "Step: 23 \t Action: [-1  0]\n",
      "Episode Reward: 0.5562510420956183\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [76.  0.] \t Current Goal State: [61 14]\n",
      "Step: 24 \t Action: [-1  0]\n",
      "Episode Reward: 0.5895732680203101\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [75.  0.] \t Current Goal State: [61 14]\n",
      "Step: 25 \t Action: [-1  0]\n",
      "Episode Reward: 0.6240441401333745\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [74.  0.] \t Current Goal State: [61 14]\n",
      "Step: 26 \t Action: [-1  0]\n",
      "Episode Reward: 0.6597456752993867\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [73.  0.] \t Current Goal State: [61 14]\n",
      "Step: 27 \t Action: [-1  0]\n",
      "Episode Reward: 0.6967689999939443\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [72.  0.] \t Current Goal State: [61 14]\n",
      "Step: 28 \t Action: [-1  0]\n",
      "Episode Reward: 0.7352157512434637\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [71.  0.] \t Current Goal State: [61 14]\n",
      "Step: 29 \t Action: [-1  0]\n",
      "Episode Reward: 0.7751997576409048\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [70.  0.] \t Current Goal State: [61 14]\n",
      "Step: 30 \t Action: [-1  0]\n",
      "Episode Reward: 0.8168490704272438\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [69.  0.] \t Current Goal State: [61 14]\n",
      "Step: 31 \t Action: [-1  0]\n",
      "Episode Reward: 0.8603084359205075\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [68.  0.] \t Current Goal State: [61 14]\n",
      "Step: 32 \t Action: [-1  0]\n",
      "Episode Reward: 0.9057423296051963\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [67.  0.] \t Current Goal State: [61 14]\n",
      "Step: 33 \t Action: [-1  0]\n",
      "Episode Reward: 0.953338712280113\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [66.  0.] \t Current Goal State: [61 14]\n",
      "Step: 34 \t Action: [-1  0]\n",
      "Episode Reward: 1.0033137247738662\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [65.  0.] \t Current Goal State: [61 14]\n",
      "Step: 35 \t Action: [-1  0]\n",
      "Episode Reward: 1.0559176174619251\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [64.  0.] \t Current Goal State: [61 14]\n",
      "Step: 36 \t Action: [-1  0]\n",
      "Episode Reward: 1.1114423259572055\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [63.  0.] \t Current Goal State: [61 14]\n",
      "Step: 37 \t Action: [-1  0]\n",
      "Episode Reward: 1.170231273635042\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [62.  0.] \t Current Goal State: [61 14]\n",
      "Step: 38 \t Action: [-1  0]\n",
      "Episode Reward: 1.2326922355338552\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [61.  0.] \t Current Goal State: [61 14]\n",
      "Step: 39 \t Action: [-1  0]\n",
      "Episode Reward: 1.2993144873659672\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [60.  0.] \t Current Goal State: [61 14]\n",
      "Step: 40 \t Action: [-1  0]\n",
      "Episode Reward: 1.3706920748035119\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [59.  0.] \t Current Goal State: [61 14]\n",
      "Step: 41 \t Action: [-1  0]\n",
      "Episode Reward: 1.4373143266356239\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [58.  0.] \t Current Goal State: [61 14]\n",
      "Step: 42 \t Action: [-1  0]\n",
      "Episode Reward: 1.499775288534437\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [57.  0.] \t Current Goal State: [61 14]\n",
      "Step: 43 \t Action: [-1  0]\n",
      "Episode Reward: 1.5585642362122736\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [56.  0.] \t Current Goal State: [61 14]\n",
      "Step: 44 \t Action: [-1  0]\n",
      "Episode Reward: 1.614088944707554\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [55.  0.] \t Current Goal State: [61 14]\n",
      "Step: 45 \t Action: [-1  0]\n",
      "Episode Reward: 1.666692837395613\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [54.  0.] \t Current Goal State: [61 14]\n",
      "Step: 46 \t Action: [-1  0]\n",
      "Episode Reward: 1.716667849889366\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [53.  0.] \t Current Goal State: [61 14]\n",
      "Step: 47 \t Action: [-1  0]\n",
      "Episode Reward: 1.7642642325642826\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [52.  0.] \t Current Goal State: [61 14]\n",
      "Step: 48 \t Action: [-1  0]\n",
      "Episode Reward: 1.8096981262489713\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [51.  0.] \t Current Goal State: [61 14]\n",
      "Step: 49 \t Action: [-1  0]\n",
      "Episode Reward: 1.853157491742235\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [50.  0.] \t Current Goal State: [61 14]\n",
      "Step: 50 \t Action: [-1  0]\n",
      "Episode Reward: 1.8948068045285742\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [49.  0.] \t Current Goal State: [61 14]\n",
      "Step: 51 \t Action: [-1  0]\n",
      "Episode Reward: 1.9347908109260152\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [48.  0.] \t Current Goal State: [61 14]\n",
      "Step: 52 \t Action: [-1  0]\n",
      "Episode Reward: 1.9732375621755347\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [47.  0.] \t Current Goal State: [61 14]\n",
      "Step: 53 \t Action: [-1  0]\n",
      "Episode Reward: 2.010260886870092\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [46.  0.] \t Current Goal State: [61 14]\n",
      "Step: 54 \t Action: [-1  0]\n",
      "Episode Reward: 2.045962422036104\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [45.  0.] \t Current Goal State: [61 14]\n",
      "Step: 55 \t Action: [-1  0]\n",
      "Episode Reward: 2.0804332941491688\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [44.  0.] \t Current Goal State: [61 14]\n",
      "Step: 56 \t Action: [-1  0]\n",
      "Episode Reward: 2.1137555200738607\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [43.  0.] \t Current Goal State: [61 14]\n",
      "Step: 57 \t Action: [-1  0]\n",
      "Episode Reward: 2.1460031821183625\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [42.  0.] \t Current Goal State: [61 14]\n",
      "Step: 58 \t Action: [-1  0]\n",
      "Episode Reward: 2.177243419544167\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [41.  0.] \t Current Goal State: [61 14]\n",
      "Step: 59 \t Action: [-1  0]\n",
      "Episode Reward: 2.207537269892546\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [40.  0.] \t Current Goal State: [61 14]\n",
      "Step: 60 \t Action: [-1  0]\n",
      "Episode Reward: 2.236940386622919\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [39.  0.] \t Current Goal State: [61 14]\n",
      "Step: 61 \t Action: [-1  0]\n",
      "Episode Reward: 2.265503654260737\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [38.  0.] \t Current Goal State: [61 14]\n",
      "Step: 62 \t Action: [-1  0]\n",
      "Episode Reward: 2.293273718131884\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [37.  0.] \t Current Goal State: [61 14]\n",
      "Step: 63 \t Action: [-1  0]\n",
      "Episode Reward: 2.320293442530695\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [36.  0.] \t Current Goal State: [61 14]\n",
      "Step: 64 \t Action: [-1  0]\n",
      "Episode Reward: 2.3466023086185666\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [35.  0.] \t Current Goal State: [61 14]\n",
      "Step: 65 \t Action: [-1  0]\n",
      "Episode Reward: 2.372236761323001\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [34.  0.] \t Current Goal State: [61 14]\n",
      "Step: 66 \t Action: [-1  0]\n",
      "Episode Reward: 2.3972305128851104\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [33.  0.] \t Current Goal State: [61 14]\n",
      "Step: 67 \t Action: [-1  0]\n",
      "Episode Reward: 2.421614809398156\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [32.  0.] \t Current Goal State: [61 14]\n",
      "Step: 68 \t Action: [-1  0]\n",
      "Episode Reward: 2.4454186656228645\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [31.  0.] \t Current Goal State: [61 14]\n",
      "Step: 69 \t Action: [-1  0]\n",
      "Episode Reward: 2.468669072504985\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [30.  0.] \t Current Goal State: [61 14]\n",
      "Step: 70 \t Action: [-1  0]\n",
      "Episode Reward: 2.491391181116664\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [29.  0.] \t Current Goal State: [61 14]\n",
      "Step: 71 \t Action: [-1  0]\n",
      "Episode Reward: 2.513608466164431\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [28.  0.] \t Current Goal State: [61 14]\n",
      "Step: 72 \t Action: [-1  0]\n",
      "Episode Reward: 2.5353428717284388\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [27.  0.] \t Current Goal State: [61 14]\n",
      "Step: 73 \t Action: [-1  0]\n",
      "Episode Reward: 2.5566149415008277\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [26.  0.] \t Current Goal State: [61 14]\n",
      "Step: 74 \t Action: [-1  0]\n",
      "Episode Reward: 2.5774439354604195\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [25.  0.] \t Current Goal State: [61 14]\n",
      "Step: 75 \t Action: [-1  0]\n",
      "Episode Reward: 2.5978479346442596\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [24.  0.] \t Current Goal State: [61 14]\n",
      "Step: 76 \t Action: [-1  0]\n",
      "Episode Reward: 2.6178439354440997\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [23.  0.] \t Current Goal State: [61 14]\n",
      "Step: 77 \t Action: [-1  0]\n",
      "Episode Reward: 2.6374479346599395\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [22.  0.] \t Current Goal State: [61 14]\n",
      "Step: 78 \t Action: [-1  0]\n",
      "Episode Reward: 2.656675006376917\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [21.  0.] \t Current Goal State: [61 14]\n",
      "Step: 79 \t Action: [-1  0]\n",
      "Episode Reward: 2.675539371591028\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [20.  0.] \t Current Goal State: [61 14]\n",
      "Step: 80 \t Action: [-1  0]\n",
      "Episode Reward: 2.6940544613892135\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [19.  0.] \t Current Goal State: [61 14]\n",
      "Step: 81 \t Action: [-1  0]\n",
      "Episode Reward: 2.71223297438685\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [18.  0.] \t Current Goal State: [61 14]\n",
      "Step: 82 \t Action: [-1  0]\n",
      "Episode Reward: 2.7300869290378054\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [17.  0.] \t Current Goal State: [61 14]\n",
      "Step: 83 \t Action: [-1  0]\n",
      "Episode Reward: 2.7476277113566967\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [16.  0.] \t Current Goal State: [61 14]\n",
      "Step: 84 \t Action: [-1  0]\n",
      "Episode Reward: 2.7648661185278742\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [15.  0.] \t Current Goal State: [61 14]\n",
      "Step: 85 \t Action: [-1  0]\n",
      "Episode Reward: 2.7818123988193504\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [14.  0.] \t Current Goal State: [61 14]\n",
      "Step: 86 \t Action: [-1  0]\n",
      "Episode Reward: 2.798476288171125\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [13.  0.] \t Current Goal State: [61 14]\n",
      "Step: 87 \t Action: [-1  0]\n",
      "Episode Reward: 2.8148670437849588\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [12.  0.] \t Current Goal State: [61 14]\n",
      "Step: 88 \t Action: [-1  0]\n",
      "Episode Reward: 2.8309934750057297\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [11.  0.] \t Current Goal State: [61 14]\n",
      "Step: 89 \t Action: [-1  0]\n",
      "Episode Reward: 2.8468639717522777\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [10.  0.] \t Current Goal State: [61 14]\n",
      "Step: 90 \t Action: [-1  0]\n",
      "Episode Reward: 2.8624865307274376\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [9. 0.] \t Current Goal State: [61 14]\n",
      "Step: 91 \t Action: [-1  0]\n",
      "Episode Reward: 2.8778687796122244\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [8. 0.] \t Current Goal State: [61 14]\n",
      "Step: 92 \t Action: [-1  0]\n",
      "Episode Reward: 2.893017999427404\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [7. 0.] \t Current Goal State: [61 14]\n",
      "Step: 93 \t Action: [-1  0]\n",
      "Episode Reward: 2.9079411452265385\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [6. 0.] \t Current Goal State: [61 14]\n",
      "Step: 94 \t Action: [-1  0]\n",
      "Episode Reward: 2.922644865267709\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [5. 0.] \t Current Goal State: [61 14]\n",
      "Step: 95 \t Action: [-1  0]\n",
      "Episode Reward: 2.9371355187961834\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [4. 0.] \t Current Goal State: [61 14]\n",
      "Step: 96 \t Action: [-1  0]\n",
      "Episode Reward: 2.9514191925570747\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [3. 0.] \t Current Goal State: [61 14]\n",
      "Step: 97 \t Action: [-1  0]\n",
      "Episode Reward: 2.965501716145302\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [2. 0.] \t Current Goal State: [61 14]\n",
      "Step: 98 \t Action: [-1  0]\n",
      "Episode Reward: 2.979388676289726\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [1. 0.] \t Current Goal State: [61 14]\n",
      "Step: 99 \t Action: [-1  0]\n",
      "Episode Reward: 2.9930854301590593\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [0. 0.] \t Current Goal State: [61 14]\n",
      "Step: 100 \t Action: [-1  0]\n",
      "Episode Reward: 3.0065971177688415\n",
      "--------------------------------------------------------------------------------------\n",
      "---------------------------\n",
      "\n",
      " NEW EPISODE \n",
      "\n",
      "---------------------------\n",
      "Current Position: [99.  0.] \t Current Goal State: [61 14]\n",
      "Step: 1 \t Action: [-1  0]\n",
      "Episode Reward: 0.013331555792560991\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [98.  0.] \t Current Goal State: [61 14]\n",
      "Step: 2 \t Action: [-1  0]\n",
      "Episode Reward: 0.032558627509538494\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [97.  0.] \t Current Goal State: [61 14]\n",
      "Step: 3 \t Action: [-1  0]\n",
      "Episode Reward: 0.052162626725378525\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [96.  0.] \t Current Goal State: [61 14]\n",
      "Step: 4 \t Action: [-1  0]\n",
      "Episode Reward: 0.07215862752521857\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [95.  0.] \t Current Goal State: [61 14]\n",
      "Step: 5 \t Action: [-1  0]\n",
      "Episode Reward: 0.0925626267090586\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [94.  0.] \t Current Goal State: [61 14]\n",
      "Step: 6 \t Action: [-1  0]\n",
      "Episode Reward: 0.11339162066865036\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [93.  0.] \t Current Goal State: [61 14]\n",
      "Step: 7 \t Action: [-1  0]\n",
      "Episode Reward: 0.1346636904410392\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [92.  0.] \t Current Goal State: [61 14]\n",
      "Step: 8 \t Action: [-1  0]\n",
      "Episode Reward: 0.15639809600504703\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [91.  0.] \t Current Goal State: [61 14]\n",
      "Step: 9 \t Action: [-1  0]\n",
      "Episode Reward: 0.1786153810528142\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [90.  0.] \t Current Goal State: [61 14]\n",
      "Step: 10 \t Action: [-1  0]\n",
      "Episode Reward: 0.20133748966449336\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [89.  0.] \t Current Goal State: [61 14]\n",
      "Step: 11 \t Action: [-1  0]\n",
      "Episode Reward: 0.2245878965466138\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [88.  0.] \t Current Goal State: [61 14]\n",
      "Step: 12 \t Action: [-1  0]\n",
      "Episode Reward: 0.2483917527713222\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [87.  0.] \t Current Goal State: [61 14]\n",
      "Step: 13 \t Action: [-1  0]\n",
      "Episode Reward: 0.27277604928436777\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [86.  0.] \t Current Goal State: [61 14]\n",
      "Step: 14 \t Action: [-1  0]\n",
      "Episode Reward: 0.29776980084647725\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [85.  0.] \t Current Goal State: [61 14]\n",
      "Step: 15 \t Action: [-1  0]\n",
      "Episode Reward: 0.323404253550912\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [84.  0.] \t Current Goal State: [61 14]\n",
      "Step: 16 \t Action: [-1  0]\n",
      "Episode Reward: 0.3497131196387836\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [83.  0.] \t Current Goal State: [61 14]\n",
      "Step: 17 \t Action: [-1  0]\n",
      "Episode Reward: 0.3767328440375947\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [82.  0.] \t Current Goal State: [61 14]\n",
      "Step: 18 \t Action: [-1  0]\n",
      "Episode Reward: 0.4045029079087416\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [81.  0.] \t Current Goal State: [61 14]\n",
      "Step: 19 \t Action: [-1  0]\n",
      "Episode Reward: 0.43306617554655935\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [80.  0.] \t Current Goal State: [61 14]\n",
      "Step: 20 \t Action: [-1  0]\n",
      "Episode Reward: 0.4624692922769328\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [79.  0.] \t Current Goal State: [61 14]\n",
      "Step: 21 \t Action: [-1  0]\n",
      "Episode Reward: 0.49276314262531207\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [78.  0.] \t Current Goal State: [61 14]\n",
      "Step: 22 \t Action: [-1  0]\n",
      "Episode Reward: 0.5240033800511165\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [77.  0.] \t Current Goal State: [61 14]\n",
      "Step: 23 \t Action: [-1  0]\n",
      "Episode Reward: 0.5562510420956183\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [76.  0.] \t Current Goal State: [61 14]\n",
      "Step: 24 \t Action: [-1  0]\n",
      "Episode Reward: 0.5895732680203101\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [75.  0.] \t Current Goal State: [61 14]\n",
      "Step: 25 \t Action: [-1  0]\n",
      "Episode Reward: 0.6240441401333745\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [74.  0.] \t Current Goal State: [61 14]\n",
      "Step: 26 \t Action: [-1  0]\n",
      "Episode Reward: 0.6597456752993867\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [73.  0.] \t Current Goal State: [61 14]\n",
      "Step: 27 \t Action: [-1  0]\n",
      "Episode Reward: 0.6967689999939443\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [72.  0.] \t Current Goal State: [61 14]\n",
      "Step: 28 \t Action: [-1  0]\n",
      "Episode Reward: 0.7352157512434637\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [71.  0.] \t Current Goal State: [61 14]\n",
      "Step: 29 \t Action: [-1  0]\n",
      "Episode Reward: 0.7751997576409048\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [70.  0.] \t Current Goal State: [61 14]\n",
      "Step: 30 \t Action: [-1  0]\n",
      "Episode Reward: 0.8168490704272438\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [69.  0.] \t Current Goal State: [61 14]\n",
      "Step: 31 \t Action: [-1  0]\n",
      "Episode Reward: 0.8603084359205075\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [68.  0.] \t Current Goal State: [61 14]\n",
      "Step: 32 \t Action: [-1  0]\n",
      "Episode Reward: 0.9057423296051963\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [67.  0.] \t Current Goal State: [61 14]\n",
      "Step: 33 \t Action: [-1  0]\n",
      "Episode Reward: 0.953338712280113\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [66.  0.] \t Current Goal State: [61 14]\n",
      "Step: 34 \t Action: [-1  0]\n",
      "Episode Reward: 1.0033137247738662\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [65.  0.] \t Current Goal State: [61 14]\n",
      "Step: 35 \t Action: [-1  0]\n",
      "Episode Reward: 1.0559176174619251\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [64.  0.] \t Current Goal State: [61 14]\n",
      "Step: 36 \t Action: [-1  0]\n",
      "Episode Reward: 1.1114423259572055\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [63.  0.] \t Current Goal State: [61 14]\n",
      "Step: 37 \t Action: [-1  0]\n",
      "Episode Reward: 1.170231273635042\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [62.  0.] \t Current Goal State: [61 14]\n",
      "Step: 38 \t Action: [-1  0]\n",
      "Episode Reward: 1.2326922355338552\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [61.  0.] \t Current Goal State: [61 14]\n",
      "Step: 39 \t Action: [-1  0]\n",
      "Episode Reward: 1.2993144873659672\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [60.  0.] \t Current Goal State: [61 14]\n",
      "Step: 40 \t Action: [-1  0]\n",
      "Episode Reward: 1.3706920748035119\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [59.  0.] \t Current Goal State: [61 14]\n",
      "Step: 41 \t Action: [-1  0]\n",
      "Episode Reward: 1.4373143266356239\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [58.  0.] \t Current Goal State: [61 14]\n",
      "Step: 42 \t Action: [-1  0]\n",
      "Episode Reward: 1.499775288534437\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [57.  0.] \t Current Goal State: [61 14]\n",
      "Step: 43 \t Action: [-1  0]\n",
      "Episode Reward: 1.5585642362122736\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [56.  0.] \t Current Goal State: [61 14]\n",
      "Step: 44 \t Action: [-1  0]\n",
      "Episode Reward: 1.614088944707554\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [55.  0.] \t Current Goal State: [61 14]\n",
      "Step: 45 \t Action: [-1  0]\n",
      "Episode Reward: 1.666692837395613\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [54.  0.] \t Current Goal State: [61 14]\n",
      "Step: 46 \t Action: [-1  0]\n",
      "Episode Reward: 1.716667849889366\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [53.  0.] \t Current Goal State: [61 14]\n",
      "Step: 47 \t Action: [-1  0]\n",
      "Episode Reward: 1.7642642325642826\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [52.  0.] \t Current Goal State: [61 14]\n",
      "Step: 48 \t Action: [-1  0]\n",
      "Episode Reward: 1.8096981262489713\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [51.  0.] \t Current Goal State: [61 14]\n",
      "Step: 49 \t Action: [-1  0]\n",
      "Episode Reward: 1.853157491742235\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [50.  0.] \t Current Goal State: [61 14]\n",
      "Step: 50 \t Action: [-1  0]\n",
      "Episode Reward: 1.8948068045285742\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [49.  0.] \t Current Goal State: [61 14]\n",
      "Step: 51 \t Action: [-1  0]\n",
      "Episode Reward: 1.9347908109260152\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [48.  0.] \t Current Goal State: [61 14]\n",
      "Step: 52 \t Action: [-1  0]\n",
      "Episode Reward: 1.9732375621755347\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [47.  0.] \t Current Goal State: [61 14]\n",
      "Step: 53 \t Action: [-1  0]\n",
      "Episode Reward: 2.010260886870092\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [46.  0.] \t Current Goal State: [61 14]\n",
      "Step: 54 \t Action: [-1  0]\n",
      "Episode Reward: 2.045962422036104\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [45.  0.] \t Current Goal State: [61 14]\n",
      "Step: 55 \t Action: [-1  0]\n",
      "Episode Reward: 2.0804332941491688\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [44.  0.] \t Current Goal State: [61 14]\n",
      "Step: 56 \t Action: [-1  0]\n",
      "Episode Reward: 2.1137555200738607\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [43.  0.] \t Current Goal State: [61 14]\n",
      "Step: 57 \t Action: [-1  0]\n",
      "Episode Reward: 2.1460031821183625\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [42.  0.] \t Current Goal State: [61 14]\n",
      "Step: 58 \t Action: [-1  0]\n",
      "Episode Reward: 2.177243419544167\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [41.  0.] \t Current Goal State: [61 14]\n",
      "Step: 59 \t Action: [-1  0]\n",
      "Episode Reward: 2.207537269892546\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [40.  0.] \t Current Goal State: [61 14]\n",
      "Step: 60 \t Action: [-1  0]\n",
      "Episode Reward: 2.236940386622919\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [39.  0.] \t Current Goal State: [61 14]\n",
      "Step: 61 \t Action: [-1  0]\n",
      "Episode Reward: 2.265503654260737\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [38.  0.] \t Current Goal State: [61 14]\n",
      "Step: 62 \t Action: [-1  0]\n",
      "Episode Reward: 2.293273718131884\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [37.  0.] \t Current Goal State: [61 14]\n",
      "Step: 63 \t Action: [-1  0]\n",
      "Episode Reward: 2.320293442530695\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [36.  0.] \t Current Goal State: [61 14]\n",
      "Step: 64 \t Action: [-1  0]\n",
      "Episode Reward: 2.3466023086185666\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [35.  0.] \t Current Goal State: [61 14]\n",
      "Step: 65 \t Action: [-1  0]\n",
      "Episode Reward: 2.372236761323001\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [34.  0.] \t Current Goal State: [61 14]\n",
      "Step: 66 \t Action: [-1  0]\n",
      "Episode Reward: 2.3972305128851104\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [33.  0.] \t Current Goal State: [61 14]\n",
      "Step: 67 \t Action: [-1  0]\n",
      "Episode Reward: 2.421614809398156\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [32.  0.] \t Current Goal State: [61 14]\n",
      "Step: 68 \t Action: [-1  0]\n",
      "Episode Reward: 2.4454186656228645\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [31.  0.] \t Current Goal State: [61 14]\n",
      "Step: 69 \t Action: [-1  0]\n",
      "Episode Reward: 2.468669072504985\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [30.  0.] \t Current Goal State: [61 14]\n",
      "Step: 70 \t Action: [-1  0]\n",
      "Episode Reward: 2.491391181116664\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [29.  0.] \t Current Goal State: [61 14]\n",
      "Step: 71 \t Action: [-1  0]\n",
      "Episode Reward: 2.513608466164431\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [28.  0.] \t Current Goal State: [61 14]\n",
      "Step: 72 \t Action: [-1  0]\n",
      "Episode Reward: 2.5353428717284388\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [27.  0.] \t Current Goal State: [61 14]\n",
      "Step: 73 \t Action: [-1  0]\n",
      "Episode Reward: 2.5566149415008277\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [26.  0.] \t Current Goal State: [61 14]\n",
      "Step: 74 \t Action: [-1  0]\n",
      "Episode Reward: 2.5774439354604195\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [25.  0.] \t Current Goal State: [61 14]\n",
      "Step: 75 \t Action: [-1  0]\n",
      "Episode Reward: 2.5978479346442596\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [24.  0.] \t Current Goal State: [61 14]\n",
      "Step: 76 \t Action: [-1  0]\n",
      "Episode Reward: 2.6178439354440997\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [23.  0.] \t Current Goal State: [61 14]\n",
      "Step: 77 \t Action: [-1  0]\n",
      "Episode Reward: 2.6374479346599395\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [22.  0.] \t Current Goal State: [61 14]\n",
      "Step: 78 \t Action: [-1  0]\n",
      "Episode Reward: 2.656675006376917\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [21.  0.] \t Current Goal State: [61 14]\n",
      "Step: 79 \t Action: [-1  0]\n",
      "Episode Reward: 2.675539371591028\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [20.  0.] \t Current Goal State: [61 14]\n",
      "Step: 80 \t Action: [-1  0]\n",
      "Episode Reward: 2.6940544613892135\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [19.  0.] \t Current Goal State: [61 14]\n",
      "Step: 81 \t Action: [-1  0]\n",
      "Episode Reward: 2.71223297438685\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [18.  0.] \t Current Goal State: [61 14]\n",
      "Step: 82 \t Action: [-1  0]\n",
      "Episode Reward: 2.7300869290378054\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [17.  0.] \t Current Goal State: [61 14]\n",
      "Step: 83 \t Action: [-1  0]\n",
      "Episode Reward: 2.7476277113566967\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [16.  0.] \t Current Goal State: [61 14]\n",
      "Step: 84 \t Action: [-1  0]\n",
      "Episode Reward: 2.7648661185278742\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [15.  0.] \t Current Goal State: [61 14]\n",
      "Step: 85 \t Action: [-1  0]\n",
      "Episode Reward: 2.7818123988193504\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [14.  0.] \t Current Goal State: [61 14]\n",
      "Step: 86 \t Action: [-1  0]\n",
      "Episode Reward: 2.798476288171125\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [13.  0.] \t Current Goal State: [61 14]\n",
      "Step: 87 \t Action: [-1  0]\n",
      "Episode Reward: 2.8148670437849588\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [12.  0.] \t Current Goal State: [61 14]\n",
      "Step: 88 \t Action: [-1  0]\n",
      "Episode Reward: 2.8309934750057297\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [11.  0.] \t Current Goal State: [61 14]\n",
      "Step: 89 \t Action: [-1  0]\n",
      "Episode Reward: 2.8468639717522777\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [10.  0.] \t Current Goal State: [61 14]\n",
      "Step: 90 \t Action: [-1  0]\n",
      "Episode Reward: 2.8624865307274376\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [9. 0.] \t Current Goal State: [61 14]\n",
      "Step: 91 \t Action: [-1  0]\n",
      "Episode Reward: 2.8778687796122244\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [8. 0.] \t Current Goal State: [61 14]\n",
      "Step: 92 \t Action: [-1  0]\n",
      "Episode Reward: 2.893017999427404\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [7. 0.] \t Current Goal State: [61 14]\n",
      "Step: 93 \t Action: [-1  0]\n",
      "Episode Reward: 2.9079411452265385\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [6. 0.] \t Current Goal State: [61 14]\n",
      "Step: 94 \t Action: [-1  0]\n",
      "Episode Reward: 2.922644865267709\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [5. 0.] \t Current Goal State: [61 14]\n",
      "Step: 95 \t Action: [-1  0]\n",
      "Episode Reward: 2.9371355187961834\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [4. 0.] \t Current Goal State: [61 14]\n",
      "Step: 96 \t Action: [-1  0]\n",
      "Episode Reward: 2.9514191925570747\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [3. 0.] \t Current Goal State: [61 14]\n",
      "Step: 97 \t Action: [-1  0]\n",
      "Episode Reward: 2.965501716145302\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [2. 0.] \t Current Goal State: [61 14]\n",
      "Step: 98 \t Action: [-1  0]\n",
      "Episode Reward: 2.979388676289726\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [1. 0.] \t Current Goal State: [61 14]\n",
      "Step: 99 \t Action: [-1  0]\n",
      "Episode Reward: 2.9930854301590593\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [0. 0.] \t Current Goal State: [61 14]\n",
      "Step: 100 \t Action: [-1  0]\n",
      "Episode Reward: 3.0065971177688415\n",
      "--------------------------------------------------------------------------------------\n",
      "---------------------------\n",
      "\n",
      " NEW EPISODE \n",
      "\n",
      "---------------------------\n",
      "Current Position: [99.  0.] \t Current Goal State: [61 14]\n",
      "Step: 1 \t Action: [-1  0]\n",
      "Episode Reward: 0.013331555792560991\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [98.  0.] \t Current Goal State: [61 14]\n",
      "Step: 2 \t Action: [-1  0]\n",
      "Episode Reward: 0.032558627509538494\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [97.  0.] \t Current Goal State: [61 14]\n",
      "Step: 3 \t Action: [-1  0]\n",
      "Episode Reward: 0.052162626725378525\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [96.  0.] \t Current Goal State: [61 14]\n",
      "Step: 4 \t Action: [-1  0]\n",
      "Episode Reward: 0.07215862752521857\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [95.  0.] \t Current Goal State: [61 14]\n",
      "Step: 5 \t Action: [-1  0]\n",
      "Episode Reward: 0.0925626267090586\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [94.  0.] \t Current Goal State: [61 14]\n",
      "Step: 6 \t Action: [-1  0]\n",
      "Episode Reward: 0.11339162066865036\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [93.  0.] \t Current Goal State: [61 14]\n",
      "Step: 7 \t Action: [-1  0]\n",
      "Episode Reward: 0.1346636904410392\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [92.  0.] \t Current Goal State: [61 14]\n",
      "Step: 8 \t Action: [-1  0]\n",
      "Episode Reward: 0.15639809600504703\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [91.  0.] \t Current Goal State: [61 14]\n",
      "Step: 9 \t Action: [-1  0]\n",
      "Episode Reward: 0.1786153810528142\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [90.  0.] \t Current Goal State: [61 14]\n",
      "Step: 10 \t Action: [-1  0]\n",
      "Episode Reward: 0.20133748966449336\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [89.  0.] \t Current Goal State: [61 14]\n",
      "Step: 11 \t Action: [-1  0]\n",
      "Episode Reward: 0.2245878965466138\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [88.  0.] \t Current Goal State: [61 14]\n",
      "Step: 12 \t Action: [-1  0]\n",
      "Episode Reward: 0.2483917527713222\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [87.  0.] \t Current Goal State: [61 14]\n",
      "Step: 13 \t Action: [-1  0]\n",
      "Episode Reward: 0.27277604928436777\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [86.  0.] \t Current Goal State: [61 14]\n",
      "Step: 14 \t Action: [-1  0]\n",
      "Episode Reward: 0.29776980084647725\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [85.  0.] \t Current Goal State: [61 14]\n",
      "Step: 15 \t Action: [-1  0]\n",
      "Episode Reward: 0.323404253550912\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [84.  0.] \t Current Goal State: [61 14]\n",
      "Step: 16 \t Action: [-1  0]\n",
      "Episode Reward: 0.3497131196387836\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [83.  0.] \t Current Goal State: [61 14]\n",
      "Step: 17 \t Action: [-1  0]\n",
      "Episode Reward: 0.3767328440375947\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [82.  0.] \t Current Goal State: [61 14]\n",
      "Step: 18 \t Action: [-1  0]\n",
      "Episode Reward: 0.4045029079087416\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [81.  0.] \t Current Goal State: [61 14]\n",
      "Step: 19 \t Action: [-1  0]\n",
      "Episode Reward: 0.43306617554655935\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [80.  0.] \t Current Goal State: [61 14]\n",
      "Step: 20 \t Action: [-1  0]\n",
      "Episode Reward: 0.4624692922769328\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [79.  0.] \t Current Goal State: [61 14]\n",
      "Step: 21 \t Action: [-1  0]\n",
      "Episode Reward: 0.49276314262531207\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [78.  0.] \t Current Goal State: [61 14]\n",
      "Step: 22 \t Action: [-1  0]\n",
      "Episode Reward: 0.5240033800511165\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [77.  0.] \t Current Goal State: [61 14]\n",
      "Step: 23 \t Action: [-1  0]\n",
      "Episode Reward: 0.5562510420956183\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [76.  0.] \t Current Goal State: [61 14]\n",
      "Step: 24 \t Action: [-1  0]\n",
      "Episode Reward: 0.5895732680203101\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [75.  0.] \t Current Goal State: [61 14]\n",
      "Step: 25 \t Action: [-1  0]\n",
      "Episode Reward: 0.6240441401333745\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [74.  0.] \t Current Goal State: [61 14]\n",
      "Step: 26 \t Action: [-1  0]\n",
      "Episode Reward: 0.6597456752993867\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [73.  0.] \t Current Goal State: [61 14]\n",
      "Step: 27 \t Action: [-1  0]\n",
      "Episode Reward: 0.6967689999939443\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [72.  0.] \t Current Goal State: [61 14]\n",
      "Step: 28 \t Action: [-1  0]\n",
      "Episode Reward: 0.7352157512434637\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [71.  0.] \t Current Goal State: [61 14]\n",
      "Step: 29 \t Action: [-1  0]\n",
      "Episode Reward: 0.7751997576409048\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [70.  0.] \t Current Goal State: [61 14]\n",
      "Step: 30 \t Action: [-1  0]\n",
      "Episode Reward: 0.8168490704272438\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [69.  0.] \t Current Goal State: [61 14]\n",
      "Step: 31 \t Action: [-1  0]\n",
      "Episode Reward: 0.8603084359205075\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [68.  0.] \t Current Goal State: [61 14]\n",
      "Step: 32 \t Action: [-1  0]\n",
      "Episode Reward: 0.9057423296051963\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [67.  0.] \t Current Goal State: [61 14]\n",
      "Step: 33 \t Action: [-1  0]\n",
      "Episode Reward: 0.953338712280113\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [66.  0.] \t Current Goal State: [61 14]\n",
      "Step: 34 \t Action: [-1  0]\n",
      "Episode Reward: 1.0033137247738662\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [65.  0.] \t Current Goal State: [61 14]\n",
      "Step: 35 \t Action: [-1  0]\n",
      "Episode Reward: 1.0559176174619251\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [64.  0.] \t Current Goal State: [61 14]\n",
      "Step: 36 \t Action: [-1  0]\n",
      "Episode Reward: 1.1114423259572055\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [63.  0.] \t Current Goal State: [61 14]\n",
      "Step: 37 \t Action: [-1  0]\n",
      "Episode Reward: 1.170231273635042\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [62.  0.] \t Current Goal State: [61 14]\n",
      "Step: 38 \t Action: [-1  0]\n",
      "Episode Reward: 1.2326922355338552\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [61.  0.] \t Current Goal State: [61 14]\n",
      "Step: 39 \t Action: [-1  0]\n",
      "Episode Reward: 1.2993144873659672\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [60.  0.] \t Current Goal State: [61 14]\n",
      "Step: 40 \t Action: [-1  0]\n",
      "Episode Reward: 1.3706920748035119\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [59.  0.] \t Current Goal State: [61 14]\n",
      "Step: 41 \t Action: [-1  0]\n",
      "Episode Reward: 1.4373143266356239\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [58.  0.] \t Current Goal State: [61 14]\n",
      "Step: 42 \t Action: [-1  0]\n",
      "Episode Reward: 1.499775288534437\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [57.  0.] \t Current Goal State: [61 14]\n",
      "Step: 43 \t Action: [-1  0]\n",
      "Episode Reward: 1.5585642362122736\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [56.  0.] \t Current Goal State: [61 14]\n",
      "Step: 44 \t Action: [-1  0]\n",
      "Episode Reward: 1.614088944707554\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [55.  0.] \t Current Goal State: [61 14]\n",
      "Step: 45 \t Action: [-1  0]\n",
      "Episode Reward: 1.666692837395613\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [54.  0.] \t Current Goal State: [61 14]\n",
      "Step: 46 \t Action: [-1  0]\n",
      "Episode Reward: 1.716667849889366\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [53.  0.] \t Current Goal State: [61 14]\n",
      "Step: 47 \t Action: [-1  0]\n",
      "Episode Reward: 1.7642642325642826\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [52.  0.] \t Current Goal State: [61 14]\n",
      "Step: 48 \t Action: [-1  0]\n",
      "Episode Reward: 1.8096981262489713\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [51.  0.] \t Current Goal State: [61 14]\n",
      "Step: 49 \t Action: [-1  0]\n",
      "Episode Reward: 1.853157491742235\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [50.  0.] \t Current Goal State: [61 14]\n",
      "Step: 50 \t Action: [-1  0]\n",
      "Episode Reward: 1.8948068045285742\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [49.  0.] \t Current Goal State: [61 14]\n",
      "Step: 51 \t Action: [-1  0]\n",
      "Episode Reward: 1.9347908109260152\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [48.  0.] \t Current Goal State: [61 14]\n",
      "Step: 52 \t Action: [-1  0]\n",
      "Episode Reward: 1.9732375621755347\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [47.  0.] \t Current Goal State: [61 14]\n",
      "Step: 53 \t Action: [-1  0]\n",
      "Episode Reward: 2.010260886870092\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [46.  0.] \t Current Goal State: [61 14]\n",
      "Step: 54 \t Action: [-1  0]\n",
      "Episode Reward: 2.045962422036104\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [45.  0.] \t Current Goal State: [61 14]\n",
      "Step: 55 \t Action: [-1  0]\n",
      "Episode Reward: 2.0804332941491688\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [44.  0.] \t Current Goal State: [61 14]\n",
      "Step: 56 \t Action: [-1  0]\n",
      "Episode Reward: 2.1137555200738607\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [43.  0.] \t Current Goal State: [61 14]\n",
      "Step: 57 \t Action: [-1  0]\n",
      "Episode Reward: 2.1460031821183625\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [42.  0.] \t Current Goal State: [61 14]\n",
      "Step: 58 \t Action: [-1  0]\n",
      "Episode Reward: 2.177243419544167\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [41.  0.] \t Current Goal State: [61 14]\n",
      "Step: 59 \t Action: [-1  0]\n",
      "Episode Reward: 2.207537269892546\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [40.  0.] \t Current Goal State: [61 14]\n",
      "Step: 60 \t Action: [-1  0]\n",
      "Episode Reward: 2.236940386622919\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [39.  0.] \t Current Goal State: [61 14]\n",
      "Step: 61 \t Action: [-1  0]\n",
      "Episode Reward: 2.265503654260737\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [38.  0.] \t Current Goal State: [61 14]\n",
      "Step: 62 \t Action: [-1  0]\n",
      "Episode Reward: 2.293273718131884\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [37.  0.] \t Current Goal State: [61 14]\n",
      "Step: 63 \t Action: [-1  0]\n",
      "Episode Reward: 2.320293442530695\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [36.  0.] \t Current Goal State: [61 14]\n",
      "Step: 64 \t Action: [-1  0]\n",
      "Episode Reward: 2.3466023086185666\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [35.  0.] \t Current Goal State: [61 14]\n",
      "Step: 65 \t Action: [-1  0]\n",
      "Episode Reward: 2.372236761323001\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [34.  0.] \t Current Goal State: [61 14]\n",
      "Step: 66 \t Action: [-1  0]\n",
      "Episode Reward: 2.3972305128851104\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [33.  0.] \t Current Goal State: [61 14]\n",
      "Step: 67 \t Action: [-1  0]\n",
      "Episode Reward: 2.421614809398156\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [32.  0.] \t Current Goal State: [61 14]\n",
      "Step: 68 \t Action: [-1  0]\n",
      "Episode Reward: 2.4454186656228645\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [31.  0.] \t Current Goal State: [61 14]\n",
      "Step: 69 \t Action: [-1  0]\n",
      "Episode Reward: 2.468669072504985\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [30.  0.] \t Current Goal State: [61 14]\n",
      "Step: 70 \t Action: [-1  0]\n",
      "Episode Reward: 2.491391181116664\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [29.  0.] \t Current Goal State: [61 14]\n",
      "Step: 71 \t Action: [-1  0]\n",
      "Episode Reward: 2.513608466164431\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [28.  0.] \t Current Goal State: [61 14]\n",
      "Step: 72 \t Action: [-1  0]\n",
      "Episode Reward: 2.5353428717284388\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [27.  0.] \t Current Goal State: [61 14]\n",
      "Step: 73 \t Action: [-1  0]\n",
      "Episode Reward: 2.5566149415008277\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [26.  0.] \t Current Goal State: [61 14]\n",
      "Step: 74 \t Action: [-1  0]\n",
      "Episode Reward: 2.5774439354604195\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [25.  0.] \t Current Goal State: [61 14]\n",
      "Step: 75 \t Action: [-1  0]\n",
      "Episode Reward: 2.5978479346442596\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [24.  0.] \t Current Goal State: [61 14]\n",
      "Step: 76 \t Action: [-1  0]\n",
      "Episode Reward: 2.6178439354440997\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [23.  0.] \t Current Goal State: [61 14]\n",
      "Step: 77 \t Action: [-1  0]\n",
      "Episode Reward: 2.6374479346599395\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [22.  0.] \t Current Goal State: [61 14]\n",
      "Step: 78 \t Action: [-1  0]\n",
      "Episode Reward: 2.656675006376917\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [21.  0.] \t Current Goal State: [61 14]\n",
      "Step: 79 \t Action: [-1  0]\n",
      "Episode Reward: 2.675539371591028\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [20.  0.] \t Current Goal State: [61 14]\n",
      "Step: 80 \t Action: [-1  0]\n",
      "Episode Reward: 2.6940544613892135\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [19.  0.] \t Current Goal State: [61 14]\n",
      "Step: 81 \t Action: [-1  0]\n",
      "Episode Reward: 2.71223297438685\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [18.  0.] \t Current Goal State: [61 14]\n",
      "Step: 82 \t Action: [-1  0]\n",
      "Episode Reward: 2.7300869290378054\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [17.  0.] \t Current Goal State: [61 14]\n",
      "Step: 83 \t Action: [-1  0]\n",
      "Episode Reward: 2.7476277113566967\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [16.  0.] \t Current Goal State: [61 14]\n",
      "Step: 84 \t Action: [-1  0]\n",
      "Episode Reward: 2.7648661185278742\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [15.  0.] \t Current Goal State: [61 14]\n",
      "Step: 85 \t Action: [-1  0]\n",
      "Episode Reward: 2.7818123988193504\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [14.  0.] \t Current Goal State: [61 14]\n",
      "Step: 86 \t Action: [-1  0]\n",
      "Episode Reward: 2.798476288171125\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [13.  0.] \t Current Goal State: [61 14]\n",
      "Step: 87 \t Action: [-1  0]\n",
      "Episode Reward: 2.8148670437849588\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [12.  0.] \t Current Goal State: [61 14]\n",
      "Step: 88 \t Action: [-1  0]\n",
      "Episode Reward: 2.8309934750057297\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [11.  0.] \t Current Goal State: [61 14]\n",
      "Step: 89 \t Action: [-1  0]\n",
      "Episode Reward: 2.8468639717522777\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [10.  0.] \t Current Goal State: [61 14]\n",
      "Step: 90 \t Action: [-1  0]\n",
      "Episode Reward: 2.8624865307274376\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [9. 0.] \t Current Goal State: [61 14]\n",
      "Step: 91 \t Action: [-1  0]\n",
      "Episode Reward: 2.8778687796122244\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [8. 0.] \t Current Goal State: [61 14]\n",
      "Step: 92 \t Action: [-1  0]\n",
      "Episode Reward: 2.893017999427404\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [7. 0.] \t Current Goal State: [61 14]\n",
      "Step: 93 \t Action: [-1  0]\n",
      "Episode Reward: 2.9079411452265385\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [6. 0.] \t Current Goal State: [61 14]\n",
      "Step: 94 \t Action: [-1  0]\n",
      "Episode Reward: 2.922644865267709\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [5. 0.] \t Current Goal State: [61 14]\n",
      "Step: 95 \t Action: [-1  0]\n",
      "Episode Reward: 2.9371355187961834\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [4. 0.] \t Current Goal State: [61 14]\n",
      "Step: 96 \t Action: [-1  0]\n",
      "Episode Reward: 2.9514191925570747\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [3. 0.] \t Current Goal State: [61 14]\n",
      "Step: 97 \t Action: [-1  0]\n",
      "Episode Reward: 2.965501716145302\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [2. 0.] \t Current Goal State: [61 14]\n",
      "Step: 98 \t Action: [-1  0]\n",
      "Episode Reward: 2.979388676289726\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [1. 0.] \t Current Goal State: [61 14]\n",
      "Step: 99 \t Action: [-1  0]\n",
      "Episode Reward: 2.9930854301590593\n",
      "--------------------------------------------------------------------------------------\n",
      "Current Position: [0. 0.] \t Current Goal State: [61 14]\n",
      "Step: 100 \t Action: [-1  0]\n",
      "Episode Reward: 3.0065971177688415\n",
      "--------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "rewards = model.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lip_env",
   "language": "python",
   "name": "lip_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
